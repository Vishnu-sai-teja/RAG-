{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing \n",
    "# -- Load the data\n",
    "# -- Split the data\n",
    "# -- store the data in the vector embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "load_dotenv()\n",
    "index_name = \"rag-model\"\n",
    "dimension_size = 768 \n",
    "\n",
    "def check_pc_index(index_name):\n",
    "    try: \n",
    "        index = pc.describe_index(index_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "pc = Pinecone(api_key = os.getenv(\"PINECONE_API_KEY\"))\n",
    "if not check_pc_index(index_name):\n",
    "    print(pc.list_indexes())\n",
    "    spec = ServerlessSpec(cloud=\"aws\",\n",
    "                          region=\"us-east-1\")\n",
    "    pc.create_index(index_name, \n",
    "                    dimension=dimension_size,\n",
    "                    spec=spec)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = PineconeVectorStore(index=index,\n",
    "                                   embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the website based docs\n",
    "# -- the soupstrainer can handle the specific elements of the web doc \n",
    "# -- -- web based loader holds the web page and loads the page \n",
    "# -- -- -- load the page into the docs object\n",
    "# -- -- Add the splits to the index \n",
    "\n",
    "\n",
    "import bs4\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    ),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\n",
    "                \"post-content\",\n",
    "                \"post-title\",\n",
    "                \"post-handler\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1215cbdd-af2a-4457-985e-f2d9c5da8826', '9cf2c357-c8c7-448e-b162-98af2f03ea72', '28c8820e-72c3-4fe2-82eb-69fd7d537b43', '2bab7791-77fb-4da7-99a0-cab03d3e166c', 'f1a63d13-d1dc-48c7-ba69-dda6eacc5977', 'c7d0624e-ec49-4a63-b5ba-c375f954238f', 'b4f3527d-2200-41f7-bc73-93d0c7eaa530', 'a96d593d-8dac-4a38-9cb1-bf59bfe4d2fe', '96c5993a-70a2-473d-badc-b07c63c48093', '67a04522-d6b7-4b27-897f-0236826e53ef', '7c720e39-f010-4e60-b103-697d429b3aa5', 'd21d0c6a-21ee-4a10-bbd1-8d0272b65700', '74da3fa6-6c5a-431e-8a9d-2c24498eb382', '606d003a-8a36-4577-a4a7-7370de85af27', 'eb324045-a48b-4332-ad4f-b03c6649e9bd', '849b7f35-a325-471b-9e02-d110effa498a', '60fe61c1-4df8-4e40-8eba-b10b671fb837', '4a0283e9-c581-4b9f-9dd1-a202203a508d', '83670fc4-e7b2-4184-840f-e5dbac05f4e1', '78122177-f4f5-4455-a84e-7b54ccf3194f', 'c724b768-b024-4bc8-9b51-909f0144e928', 'a985dae6-0757-40dd-9a2c-f8696ec27eae', '39a83474-31d5-4452-be40-e161d69669c4', 'b69b3e18-470e-4a7c-8bdb-da102d702234', '006659fe-eb2a-461b-bed5-8af8e3a40489', '641e7052-4739-478d-846c-df097c874992', 'eb70b40b-8011-44d1-8097-c555fa6ce046', '140c2e39-f384-4465-996e-b637e4d4fc59', '6d628be6-d9d8-4cdc-ba97-d036f77ca753', 'e029d850-8c94-4e82-8359-f75d6eeca748', 'a0824601-077a-44d0-8a58-8e0170189dc6', '892551b6-7ac2-418f-a6f2-864583f8ba4e', '18779f87-0694-490c-ac89-f5b7db672449', 'be9ab7e9-dae3-4a56-b285-2403c60e88c3', '0d651d99-c725-4f96-91e1-52dcd8474dd2', '489ed809-48c1-4589-8981-1fb2a9a68e06', '7c2919c7-a460-4ad9-8413-87bd55e24a2c', '6b2ffd62-b942-4c01-a2b5-7a6660e3fd2f', '2b1dbefc-2469-4117-8e68-31c675078278', '4c9c15ab-e90e-4e6b-bb85-247ea036361f', 'fedafcef-35d7-4f93-b427-31b58adff157', 'fff9f208-5b31-429f-9e56-6aaab1dca01f', '21d7dfeb-4e05-4a4d-90fd-74f1c544fab1', 'a4125273-e5bb-4404-b683-e34d95ea57f9', '2ca5b23a-302f-43ba-912c-a37a8c2827c1', '9ae7fe31-1d99-48c7-ba41-1531d58f6db9', '0c716c1c-f3e2-4752-8368-a5518cc1ba1d', 'cc6d2a84-f9f3-4c4e-aaaf-816025e0c421', 'f1d73bdc-6b8d-40b3-a3a6-909dd2b56197', '1081a83f-6fd9-4c2a-b16c-6f4069a54954', '9a63337f-ab20-4f94-b992-868a96470b2d', '9cdde600-28bb-4d6e-93be-a5e25bf5286c', 'd69837b2-04ed-4077-a486-06eaa1f7e580', '7a1a95bf-c383-48b9-af64-fd75f0e58126', '63ac1b12-801e-4c3f-8168-e16a478e7702', '5e13a629-0f04-4c7b-a46f-320a41da037d', '969a3dd5-67f2-4348-a472-9fc8010d25e7', 'ae42f5f4-aad5-402d-b23f-da9cde5f924d', 'dfdbfdbb-6180-4c38-a999-5ac0cf01b57e', '6e02cbaa-bcfd-4eee-a2b4-ecf740f7a666', '0722318c-2a90-4677-b20b-02a6c204b570', '8447396a-061b-4363-bb8a-4a584e89f250', '84042083-a15c-4cc4-a555-bb304c4301fd', '33104107-ddeb-492b-b389-6a4a2d1be7af', '1ef80237-71e7-4cfb-b87e-cfea66ca780b', '539cc346-4af1-458c-95b4-e9c3e234983c']\n"
     ]
    }
   ],
   "source": [
    "# Store the vectors into DB \n",
    "doc_ids = vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishnusaitejanagabandi/Documents/RAG-/venv/lib/python3.10/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: Here is the question for you to answer \n",
      "Context: Here is the context for the question \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Retrival and generation\n",
    "\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "message = prompt.invoke({\n",
    "    \"context\" : \"Here is the context for the question\",\n",
    "    \"question\" : \"Here is the question for you to answer\"\n",
    " }).to_messages()\n",
    "print(message[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use langgraph to tie the retrival and generation together\n",
    "# -- Define a state that can be carried through different agents\n",
    "\n",
    "from langchain_core.documents import Document \n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retriver(state: State):\n",
    "    retrived_docs = vector_store.similarity_search(state['question'])\n",
    "    return {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": retrived_docs\n",
    "    }\n",
    "\n",
    "def generation(state: State):\n",
    "    docs_content = '\\n\\n'.join([doc.page_content for doc in state['context']])\n",
    "    messages = prompt.invoke({\n",
    "        \"question\": state['question'],\n",
    "        \"context\": docs_content\n",
    "    })\n",
    "    response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"question\": state['question'],\n",
    "        \"context\": docs_content,\n",
    "        \"answer\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a graph out of the above retrival and augumentation\n",
    "# -- using a sequence for the simplicity\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retriver, generation])\n",
    "graph_builder.add_edge(START, \"retriver\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "Answer: content='Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks. It can be done using large language models (LLM) with simple prompting, task-specific instructions, or human inputs. This helps in creating a tree structure of thought steps, allowing for more efficient problem-solving.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 678, 'total_tokens': 740, 'completion_time': 0.082666667, 'prompt_time': 0.045098973, 'queue_time': 0.021296797000000006, 'total_time': 0.12776564}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None} id='run-bc9a3215-38fb-4d74-bd78-7449f8e74703-0' usage_metadata={'input_tokens': 678, 'output_tokens': 62, 'total_tokens': 740}\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png())) - display the simple graph \n",
    "\n",
    "result = graph.invoke({\n",
    "    \"question\": \"What is task decomposition\"\n",
    "})\n",
    "print(f\"Context: {result['context']}\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : {'retriver': {'question': 'What is task decomposition ?', 'context': [Document(id='2bab7791-77fb-4da7-99a0-cab03d3e166c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='6d628be6-d9d8-4cdc-ba97-d036f77ca753', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\"), Document(id='641e7052-4739-478d-846c-df097c874992', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:'), Document(id='8447396a-061b-4363-bb8a-4a584e89f250', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.')]}} \n",
      "\n",
      "\n",
      "\n",
      "Step : {'generation': {'question': 'What is task decomposition ?', 'context': 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.', 'answer': AIMessage(content='Task decomposition is the process of breaking down a complex task into smaller, manageable subtasks. It can be done by Large Language Models (LLMs) with simple prompting, using task-specific instructions, or with human inputs. This process involves identifying the subgoals and steps necessary to achieve the main task, creating a tree-like structure of thought steps.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 602, 'total_tokens': 672, 'completion_time': 0.093333333, 'prompt_time': 0.040169161, 'queue_time': 0.020425621999999997, 'total_time': 0.133502494}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-340d4d8a-b217-44f6-a9a1-dbcfc8f7389a-0', usage_metadata={'input_tokens': 602, 'output_tokens': 70, 'total_tokens': 672})}} \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streaming a graph \n",
    "for step in graph.stream(\n",
    "    {\"question\": \"What is task decomposition ?\"}, \n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"Step : {step} \\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt model \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'text': 'LLM Powered Autonomous Agents\\n    Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory',\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query Analysis\n",
    "# -- one step ahead from brute force \n",
    "# -- -- create multiple queries from the actual input query\n",
    "# -- create metadata for teh documents \n",
    "\n",
    "total_documents = len(splits)\n",
    "third = total_documents // 3 \n",
    "\n",
    "for index, document in enumerate(splits):\n",
    "    if index < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif index < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"enc\"\n",
    "    \n",
    "splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'text': 'LLM Powered Autonomous Agents\\n    Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory',\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key = os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"rag-model-2\"\n",
    "if not check_pc_index(index_name):\n",
    "    spec = ServerlessSpec(cloud=\"aws\",\n",
    "                          region=\"us-east-1\")\n",
    "    pc.create_index(index_name, \n",
    "                    dimension=dimension_size,\n",
    "                    spec=spec)\n",
    "    \n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(embedding=embeddings,\n",
    "                                   index=index)\n",
    "ids = vector_store.add_documents(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "class Search(TypedDict):\n",
    "    # \"\"\"\n",
    "    #    query with out the section and summrised query, and section in the query\n",
    "    # \"\"\"\n",
    "    \"\"\"Search query.\"\"\"\n",
    "    query: Annotated[str, ..., \"Search Query to run \"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"section to query\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "def filter_by_section(doc, section) -> bool:\n",
    "    print(doc, section)\n",
    "    doc.metadata.get(\"section\") == section\n",
    "\n",
    "def retriver_new(state: State):\n",
    "    query = state[\"query\"]\n",
    "    # print(vector_store.similarity_search(\n",
    "    #     query[\"query\"],\n",
    "    #     filter= lambda doc: filter_by_section(doc, query['section'])\n",
    "    # ))\n",
    "    retrived_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter={\n",
    "            \"section\": query['section']\n",
    "        },\n",
    "        k=3\n",
    "    )\n",
    "    # print(context)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"context\": retrived_docs\n",
    "    }\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query,retriver_new, generation])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'question': 'What does the end of the post say about Task Decomposition?', 'query': {'query': 'Task Decomposition', 'section': 'end'}}}.....\n",
      "\n",
      "\n",
      "{'retriver_new': {'query': {'query': 'Task Decomposition', 'section': 'end'}, 'context': []}}.....\n",
      "\n",
      "\n",
      "{'generation': {'question': 'What does the end of the post say about Task Decomposition?', 'context': '', 'answer': AIMessage(content='However, there is no context provided. Please provide the context, and I will be happy to assist you in answering your question.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 106, 'total_tokens': 133, 'completion_time': 0.041344957, 'prompt_time': 0.011093972, 'queue_time': 0.018199255, 'total_time': 0.052438929}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-3d7e32d1-e955-4941-955e-57d88b709cc4-0', usage_metadata={'input_tokens': 106, 'output_tokens': 27, 'total_tokens': 133})}}.....\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}.....\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
